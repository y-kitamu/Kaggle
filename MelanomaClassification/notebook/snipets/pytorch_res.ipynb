{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At least fixing some random seeds. \n",
    "# It is still impossible to make results 100% reproducible when using GPU\n",
    "warnings.simplefilter('ignore')\n",
    "torch.manual_seed(47)\n",
    "np.random.seed(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, filename: str, data: np.ndarray, train: bool = True, transforms = None):\n",
    "        \"\"\"\n",
    "        Class initialization\n",
    "        Args:\n",
    "            df (str): path to pandas.DataFrame \n",
    "            data (np.ndarray): resized images data in a shape of (HxWxC)\n",
    "            train (bool): flag of whether a training dataset is being initialized or testing one\n",
    "            transforms: image transformation method to be applied\n",
    "            \n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.y = self.df['target'].values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            x = transforms.ToPILImage()(x)\n",
    "            x = self.transforms(x)\n",
    "\n",
    "        assert x.shape == (3, 224, 224)\n",
    "        assert x.dtype == torch.float32\n",
    "        \n",
    "        if self.train:\n",
    "            y = self.df.iloc[index]['target']\n",
    "            assert y == 0 or y == 1\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, arch):\n",
    "        super(Net, self).__init__()\n",
    "        self.arch = arch\n",
    "        #self.arch._fc = nn.Linear(in_features=arch._fc.in_features, out_features=1, bias=True)\n",
    "        self.arch.fc = nn.Linear(in_features=arch.fc.in_features, out_features=1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n",
    "        Which applies sigmoid for us when calculating a loss\n",
    "        \"\"\"\n",
    "        x = self.arch(x)\n",
    "        return x\n",
    "    \n",
    "class ArchFaceNet(nn.Module):\n",
    "    def __init__(self, arch):\n",
    "        super(ArchFaceNet, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.arch._fc = nn.Linear(in_features=arch._fc.in_features, out_features=1, bias=False)\n",
    "\n",
    "    def forward_resnet(self, x):\n",
    "        x = self.arch.conv1(x)\n",
    "        x = self.arch.bn1(x)\n",
    "        x = self.arch.relu(x)\n",
    "        x = self.arch.maxpool(x)\n",
    "\n",
    "        x = self.arch.layer1(x)\n",
    "        x = self.arch.layer2(x)\n",
    "        x = self.arch.layer3(x)\n",
    "        x = self.arch.layer4(x)\n",
    "\n",
    "        x = self.arch.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = x / x.square().sum(1, keepdim=True)\n",
    "        self.arch.fc.weight = self.arch.fc.weight / self.arch.fc.weight.square().sum(1, keepdim=True)\n",
    "        x = self.arch.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.arch.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = self.arch._avg_pooling(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.arch._dropout(x)\n",
    "        \n",
    "        x = x / x.square().sum(1, keepdim=True)\n",
    "        self.arch._fc.weight = nn.parameter.Parameter(self.arch._fc.weight / self.arch._fc.weight.square().sum(1, keepdim=True))\n",
    "        x = self.arch._fc(x)\n",
    "        return x\n",
    "    \n",
    "class BinaryArchFaceLoss(nn.Module):\n",
    "    def __init__(self, m=math.pi / 6, s=2):\n",
    "        super().__init__()\n",
    "        assert m > 0\n",
    "        assert s > 1\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        x = torch.acos(inputs)\n",
    "        x = torch.cos(x + targets * self.m) * self.s\n",
    "        loss = F.binary_cross_entropy_with_logits(x, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MelanomaDataset(filename='/home/kitamura/dataset/Melanoma/train.csv', \n",
    "                        data=np.load('x_train_224.npy'),\n",
    "                        transforms=transform)\n",
    "test = MelanomaDataset(filename='/home/kitamura/dataset/Melanoma/test.csv', \n",
    "                       data=np.load('x_test_224.npy'), \n",
    "                       train=False,\n",
    "                       transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs, model, optim, criterion, train_loader, val_loader, es_patience, model_path):\n",
    "    best_val = 0.0  # Best validation score within this fold\n",
    "    patience = es_patience  # Current patience counter\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            assert model.training == True\n",
    "            x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "            optim.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
    "            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n",
    "            epoch_loss += loss.item()\n",
    "        train_acc = correct / len(train_idx)\n",
    "\n",
    "        model.eval()  # switch model to the evaluation mode\n",
    "        val_preds = torch.zeros((len(val_loader.dataset), 1), dtype=torch.float32, device=device)\n",
    "        val_true = np.zeros(len(val_loader.dataset), dtype=np.int)\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
    "            # Predicting on validation set\n",
    "            for j, (x_val, y_val) in enumerate(val_loader):\n",
    "                assert model.training == False\n",
    "                val_true[j*x_val.shape[0]:(j + 1)*x_val.shape[0]] = y_val\n",
    "                x_val = torch.tensor(x_val, device=device, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "                z_val = model(x_val)\n",
    "                val_loss = criterion(z_val, y_val.unsqueeze(1))\n",
    "                val_pred = torch.sigmoid(z_val)\n",
    "                val_preds[j*x_val.shape[0]:(j + 1)*x_val.shape[0]] = val_pred\n",
    "            val_acc = accuracy_score(val_true, torch.round(val_preds.cpu()))\n",
    "            val_roc = roc_auc_score(val_true, val_preds.cpu())\n",
    "            \n",
    "        print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val loss: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n",
    "            epoch + 1, epoch_loss, train_acc, val_acc, val_loss, val_roc, str(datetime.timedelta(seconds=time.time() - start_time))))\n",
    "            \n",
    "        if val_roc >= best_val:\n",
    "            best_val = val_roc\n",
    "            patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
    "            torch.save(model, model_path)  # Saving current best model\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model_path = 'model.pth'\n",
    "es_patience = 3\n",
    "batch_size = 70\n",
    "skf = StratifiedKFold(n_splits=5, random_state=47, shuffle=True)\n",
    "preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X=np.zeros(len(train)), y=train.y), 1):\n",
    "    print('=' * 20, 'Fold', fold, '=' * 20)\n",
    "    \n",
    "#     arch = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "#     model = Net(arch=arch)\n",
    "    #model = ArchFaceNet(arch=arch)\n",
    "    arch = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
    "    model = Net(arch=arch)\n",
    "     \n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam([\n",
    "        {\"params\": model.arch.layer2.parameters(), \"lr\": 0.00001},\n",
    "        {\"params\": model.arch.layer3.parameters(), \"lr\": 0.0001},\n",
    "        {\"params\": model.arch.layer4.parameters(), \"lr\": 0.0001},\n",
    "        {\"params\": model.arch.fc.parameters(), \"lr\": 0.001},\n",
    "    ])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = BinaryArchFaceLoss()\n",
    "    #criterion =FocalLoss()\n",
    "    train_loader = DataLoader(dataset=Subset(train, train_idx), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=Subset(train, val_idx), batch_size=50, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test, batch_size=50, shuffle=False)\n",
    "    \n",
    "    run_train(epochs, model, optim, criterion, train_loader, val_loader, es_patience, model_path)\n",
    "        \n",
    "    model = torch.load(model_path)  # Loading best model of this fold\n",
    "    model.eval()  # switch model to the evaluation mode\n",
    "    val_preds = torch.zeros((len(val_idx), 1), dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        for i, x_test in enumerate(test_loader):\n",
    "            x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "            z_test = model(x_test)\n",
    "            z_test = torch.sigmoid(z_test)\n",
    "            preds[i*x_test.shape[0]:i*x_test.shape[0] + x_test.shape[0]] += z_test / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/home/kitamura/dataset/Melanoma/sample_submission.csv')\n",
    "sub['target'] = preds.cpu().numpy()\n",
    "sub.to_csv('submission_res50_finetune.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
