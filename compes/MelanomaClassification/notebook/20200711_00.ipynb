{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 20200711_00\n",
    "augmentation : flip, noise  \n",
    "meta data を追加  \n",
    "20200630_00からデータセット周りのバグ修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainercv\n",
    "from chainer.dataset import convert\n",
    "import numpy as np\n",
    "\n",
    "import mython\n",
    "import melanoma\n",
    "from melanoma import augmentations\n",
    "\n",
    "title = \"20200711_00\"\n",
    "\n",
    "device = 0\n",
    "batch_size = 32\n",
    "width = 336\n",
    "height = 224\n",
    "epoch = 10\n",
    "n_classes = 2\n",
    "output_dir = melanoma.constants.PROJECT_ROOT / \"results\" / title\n",
    "transfered_weights = melanoma.constants.PROJECT_ROOT / \"src\" / \"melanoma\" / \"models\" / \"weights\" / \"efficientnetb0_chainer.npz\"\n",
    "n_folds = 5\n",
    "n_ensemble_loop = 1\n",
    "augs = [augmentations.base.standard_aug_transform]\n",
    "trans = [augmentations.base.normalize_transform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run train\n",
    "import cProfile\n",
    "\n",
    "for loop in range(n_ensemble_loop):\n",
    "    dataset_gen = melanoma.dataset.DatasetBuilder(\n",
    "        img_size=(width, height), n_classes=n_classes, random_state=loop, augmentations=augs, transforms=trans, is_onehot=True,\n",
    "    ).get_cross_validation_dataset_generator(n_folds)\n",
    "\n",
    "    for idx, (train_ds, val_ds, test_ds) in enumerate(dataset_gen):\n",
    "        print(f\"\\nstart cross validation : {idx + 1} / {n_folds}, loop : {loop + 1} / {n_ensemble_loop} \\n\")\n",
    "        \n",
    "        # Training\n",
    "        cross_val_output_dir = output_dir / f\"{loop * n_folds + idx:02d}\"\n",
    "        train_itr = chainer.iterators.MultiprocessIterator(train_ds, batch_size, repeat=True)\n",
    "        val_itr = chainer.iterators.MultiprocessIterator(val_ds, batch_size, repeat=False)\n",
    "        test_itr = chainer.iterators.MultiprocessIterator(test_ds, batch_size, repeat=False)\n",
    "\n",
    "        extractor = melanoma.models.EfficientNet(num_classes=1, global_params=melanoma.models.EfficientNetB0)\n",
    "        #model = extractor\n",
    "        model = melanoma.models.Net(extractor, len(melanoma.dataset.Dataset.METAFEATURES))\n",
    "        if transfered_weights is not None:\n",
    "            #chainer.serializers.load_npz(transfered_weights, model)\n",
    "            mython.ml.load_weights.load_weights(np.load(transfered_weights), extractor)\n",
    "        \n",
    "        if device >= 0:\n",
    "            model.to_gpu(device)\n",
    "            \n",
    "        loss_func = melanoma.models.loss.SigmoidLoss(model)\n",
    "        predictor = melanoma.predictor.Predictor(model, (width, height))\n",
    "\n",
    "        optimizer = chainer.optimizers.Adam(alpha=0.0001)\n",
    "        optimizer.setup(model)\n",
    "\n",
    "        updater = chainer.training.StandardUpdater(\n",
    "            train_itr, optimizer, converter=convert.concat_examples, loss_func=loss_func, device=device\n",
    "        )\n",
    "        evaluator = melanoma.evaluator.CustomEvaluator(\n",
    "            val_itr, model, device=device, eval_func=loss_func\n",
    "        )\n",
    "        trainer = melanoma.trainer.TrainerBuilder(updater, epoch ,evaluator, cross_val_output_dir).build()\n",
    "\n",
    "        profiler = cProfile.Profile()\n",
    "        mython.debug.start_pdb(\n",
    "            lambda: profiler.runcall(trainer.run)\n",
    "        )\n",
    "\n",
    "        profiler.dump_stats(cross_val_output_dir / \"profile.stat\".format(idx))\n",
    "\n",
    "        # Evaluation\n",
    "        npz_footer = \"accuracy\"\n",
    "        fname = cross_val_output_dir / f\"snapshot_model_{npz_footer}.npz\"\n",
    "        chainer.serializers.load_npz(fname, model)\n",
    "        mython.debug.start_pdb(\n",
    "            lambda : melanoma.evaluate.evaluate(predictor, \n",
    "                                   test_itr,\n",
    "                                   [l.name for l in melanoma.constants.Labels],\n",
    "                                   cross_val_output_dir / f\"{title}_eval_{npz_footer}\",\n",
    "                                   device=device,\n",
    "                                  )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "npz_footer = \"accuracy\"\n",
    "files = sorted(output_dir.glob(\"*/snapshot_model_{}.npz\".format(npz_footer)))\n",
    "\n",
    "model = melanoma.models.Net(melanoma.models.EfficientNet(), len(melanoma.dataset.Dataset.METAFEATURES))\n",
    "predictor = melanoma.predictor.Predictor(model, (width, height))\n",
    "ds = melanoma.dataset.SubmissionDataset(melanoma.dataset.DATASET_ROOT / \"test.csv\")\n",
    "itr = chainer.iterators.MultiprocessIterator(ds, batch_size, repeat=False)\n",
    "itr.reset()\n",
    "mython.debug.start_pdb(\n",
    "lambda: melanoma.evaluate.evaluate_submission(predictor, \n",
    "                                              itr,\n",
    "                                              output_dir / f\"{title}_submission_{npz_footer}\",\n",
    "                                              device=device,\n",
    "                                              filenames=files,\n",
    "                                             )\n",
    "#    lambda: melanoma.evaluate._sum_predict(output_dir.glob(\"*.csv\"), output_dir / f\"{title}_submission_{npz_footer}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
