{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainercv import transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(chainer.dataset.DatasetMixin):\n",
    "    def __init__(self, df, data, train=True, transform=None):\n",
    "        self.df = pd.read_csv(df)\n",
    "        self.data = data.transpose(0, 3, 1, 2).astype(np.float32)\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.y = self.df[\"target\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "            \n",
    "    def get_example(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        x = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        if self.train:\n",
    "            y = row[\"target\"]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class Net(chainer.Chain):\n",
    "    def __init__(self, arch):\n",
    "        super(Net, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.arch = arch\n",
    "            self.arch.fc6 = L.Linear(None, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.arch.extract(x)[\"pool5\"]\n",
    "        x = self.arch.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    \n",
    "    def __init__(self, model, gamma=2.0):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        x = self.model.forward(*args[:-1])\n",
    "        loss = self._loss_func(x, args[-1])\n",
    "        return loss\n",
    "    \n",
    "    def _loss_func_focal(self, x, t):\n",
    "        ce_loss = F.sigmoid_cross_entropy(x, t[..., None], reduce='no')\n",
    "        pt = F.exp(-ce_loss)\n",
    "        self.loss = F.mean((1 - pt)**self.gamma * ce_loss)\n",
    "        self.accuracy = F.binary_accuracy(x.array.flatten(), t)\n",
    "        return self.loss\n",
    "\n",
    "    def _loss_func(self, x, t):\n",
    "        self.loss = F.sigmoid_cross_entropy(x, t[..., None], reduce='mean')\n",
    "        self.accuracy = F.binary_accuracy(x.array.flatten(), t)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = Dataset(filename=Path.home() / \"dataset/Melanoma/train.csv\")\n",
    "train_ds = Dataset(df=Path.home() / \"dataset/Melanoma/train.csv\",\n",
    "                   data=np.load(\"x_train_224.npy\"))\n",
    "test_ds = Dataset(df=Path.home() / \"dataset/Melanoma/test.csv\",\n",
    "                  data=np.load(\"x_test_224.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(train_df)\n",
    "train_itr = chainer.iterators.MultiprocessIterator(train_ds, 30, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_itr.reset()\n",
    "batch = next(train_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 30\n",
    "device = 0\n",
    "title = \"test\"\n",
    "output_dir = Path(\"../../results\") / title\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "es_patience = 3  # Early Stopping patience\n",
    "skf = StratifiedKFold(n_splits=5, random_state=47, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X=train_df, y=train_df.target), 1):\n",
    "    print('=' * 20, 'Fold', fold, '=' * 20)\n",
    "    best_val = None\n",
    "    patience = es_patience\n",
    "    \n",
    "    train_ds = Dataset(train_df.iloc[train_idx])\n",
    "    val_ds = Dataset(train_df.iloc[val_idx])\n",
    "    train_itr = chainer.iterators.MultiprocessIterator(train_ds, batch_size, repeat=False, shuffle=True)\n",
    "    val_itr = chainer.iterators.MultiprocessIterator(val_ds, batch_size, repeat=False, shuffle=False)\n",
    "    \n",
    "    arch = chainer.links.ResNet50Layers()\n",
    "    model = Net(arch)\n",
    "    if device >= 0:\n",
    "        model.to_gpu(device)\n",
    "    \n",
    "    optimizer = chainer.optimizers.Adam(alpha=0.001)\n",
    "    optimizer.setup(model)\n",
    "    loss_func = Loss(model)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        train_itr.reset()\n",
    "        val_itr.reset()\n",
    "        \n",
    "        with chainer.using_config(\"train\", True):\n",
    "            for batch in train_itr:\n",
    "                x = [img for img, _ in batch]\n",
    "                t = chainer.dataset.to_device(device, np.array([label for _, label in batch]))\n",
    "                optimizer.update(loss_func, x, t)\n",
    "                epoch_loss += loss_func.loss.item()\n",
    "                print(\"\\r{} / {} : [loss = {:.3f}, acc = {:.3f}]\".format(\n",
    "                    train_itr.current_position, train_itr._epoch_size, \n",
    "                    loss_func.loss.item(), loss_func.accuracy.item()), end=\"\")\n",
    "            print(\"finish training\")\n",
    "            \n",
    "        with chainer.using_config(\"train\", False):\n",
    "            val_preds = np.zeros(len(val_ds))\n",
    "            for idx, batch in enumerate(val_itr):\n",
    "                x = [img for img, _ in batch]\n",
    "                t = chainer.dataset.to_device(device, np.array([label for _, label in batch]))\n",
    "                pred = cp.asnumpy(F.sigmoid(model(x)).array).flatten()\n",
    "                val_preds[idx * len(batch): (idx + 1) * len(batch)] = pred\n",
    "            val_accuracy = accuracy_score(val_ds.y, np.round(val_preds))\n",
    "            val_roc = roc_auc_score(val_ds.y, val_preds)\n",
    "            \n",
    "            print('Epoch {:03}: | Train Loss: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | time : {}'.format(\n",
    "                epoch + 1, epoch_loss, val_accuracy, val_roc, str(datetime.timedelta(seconds=time.time() - start_time))))\n",
    "\n",
    "            if not best_val:\n",
    "                best_val = val_roc\n",
    "                chainer.serializers.save_npz(str(output_dir / f\"roc_{fold:02}.npz\"), model)\n",
    "            elif val_roc > best_val - 1.e-5:\n",
    "                best_val = val_roc\n",
    "                patience = es_patience\n",
    "                chainer.serializers.save_npz(str(output_dir / f\"roc_{fold:02}.npz\"), model)\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(\"Early stopping!\")\n",
    "                    continue\n",
    "    chainer.serializers.save_npz(str(output_dir / f\"epoch_{fold:02}.npz\"), model)\n",
    "    print(\"Finish Fold{}. Best Val roc_aud : {:.3f}\".format(fold, best_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X=train_df, y=train_df.target), 1):\n",
    "    test_ds = Dataset(test_df, train=False)\n",
    "    test_itr = chainer.iterators.MultiprocessIterator(test_ds, batch_size, shuffle=False, repeat=False)\n",
    "    \n",
    "    arch = chainer.links.ResNet50Layers()\n",
    "    model = Net(arch)\n",
    "    chainer.serializers.load_npz(str(output_dir / f\"roc_{fold:02}.npz\"), model)\n",
    "    \n",
    "    preds = np.zeros(len(test_ds))\n",
    "    for idx, batch in enumerate(test_itr):\n",
    "        batch = chainer.datasets.concat_example(batch, device)\n",
    "        pred = F.sigmoid(model.forward(batch))\n",
    "        preds[idx * batch_size:(idx + 1) * batch_size] += pred / skf.n_splits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
